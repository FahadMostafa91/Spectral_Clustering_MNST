# -*- coding: utf-8 -*-
"""Example_Spectral _Clustering_MNST.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ONMenJFe3K5xv1QetTn8jlnooD9PG15Z

#Spectral Clustering
We give some examples of spectral clustering here. First, we code the binary version of the algorithm, and copy the kmeans code from last time.
"""

import numpy as np
from scipy import sparse
import matplotlib.pyplot as plt

def binary_spectral_clustering(W,plot_clustering=False):
    """
    Binary Spectral Clustering

    Args:
        W: nxn weight matrix
        plot_clustering: Whether to scatter plot clustering

    Returns:
        Numpy array of labels obtained by binary spectral clustering
    """
    #Graph Laplacian
    n = W.shape[0]
    d = W@np.ones(n)
    L = sparse.spdiags(d,0,n,n) - W

    #Find Fiedler vector
    vals, vec = sparse.linalg.eigsh(L,k=2,which='SM')
    v = vec[:,1]

    #Cluster labels
    labels = v > 0

    #Plot clustering
    if plot_clustering:
        plt.figure()
        plt.scatter(X[:,0],X[:,1],c=labels)
        plt.title('Spectral Clustering')

    return labels


def kmeans(X,k,plot_clustering=False,T=20):
    """
    k-means Clustering

    Args:
        X: nxm array of data, each row is a datapoint
        k: Number of clusters
        plot_clustering: Whether to plot final clustering
        T: Max number of iterations

    Returns:
        Numpy array of labels obtained by binary k-means clustering
    """

    #Number of data points
    n = X.shape[0]

    #Randomly choose initial cluster means
    means = X[np.random.choice(n,size=k,replace=False),:]

    #Initialize arrays for distances and labels
    dist = np.zeros((k,n))
    labels = np.zeros((n,))

    #Main iteration for kmeans
    num_changed = 1
    i=0
    while i < T and num_changed > 0:

        #Update labels
        old_labels = labels.copy()
        for j in range(k):
            dist[j,:] = np.sum((X - means[j,:])**2,axis=1)
        labels = np.argmin(dist,axis=0)
        num_changed = np.sum(labels != old_labels)

        #Update means
        for j in range(k):
            means[j,:] = np.mean(X[labels==j,:],axis=0)

        #Iterate counter
        i+=1

        #Plot result (red points are labels)
    if plot_clustering:
        plt.scatter(X[:,0],X[:,1], c=labels)
        plt.scatter(means[:,0],means[:,1], c='r')
        plt.title('K-means clustering')

    return labels

"""Let's now test the algorithm on one of the examples that k-means did poorly on."""

import sklearn.datasets as datasets

n=500
X,L = datasets.make_moons(n_samples=n,noise=0.1)

#kmeans clustering
kmeans_labels = kmeans(X,2,plot_clustering=True)

#Spectral clustering
sigma = 0.15
n = X.shape[0]
I = np.zeros((n,n), dtype=int)+np.arange(n, dtype=int)
dist = np.sum((X[I,:] - X[I.T,:])**2,axis=2)
W = np.exp(-dist/(2*sigma**2))
spectral_labels = binary_spectral_clustering(W,plot_clustering=True)

"""#Spectral clustering on MNIST
Let's now try spectral clustering on MNIST.
"""

pip install -q graphlearning annoy

"""The GraphLearning package contains useful functions for building large sparse graphs over common datasets, including MNIST, and for running common machine learning algorithms, like specral clustering. The code below runs spectral clustering on a pair of digits from MNIST. First we load MNIST and display some images."""

import graphlearning as gl

#Load MNIST data and labels and plot some images
data, labels = gl.datasets.load('MNIST')
gl.utils.image_grid(data, n_rows=20, n_cols=20, title='Some MNIST Images', fontsize=26)

import numpy as np

#Binary clustering problem witih 2 digits
class1 = 5
class2 = 8

#Subset data to two digits
I = labels == class1
J = labels == class2
X = data[I | J,:]
L = labels[I | J]

#Convert labels to 0/1
I = L == class1
L[I] = 0
L[~I] = 1

#Build Graph (sparse 10-nearest neighbor graph)
W = gl.weightmatrix.knn(X,10)

class1 = 5
class2 = 8

#Subset data to two digits
I = labels == class1
J = labels == class2
X = data[I | J,:]
Y = labels[I | J]


G = gl.graph(W)

#Compute eigenvectors of graph Laplacian
vals, vecs = G.eigen_decomp(normalization='normalized', k=20)

#Plot spectral embedding colored by label
#2D plot
plt.figure(figsize=(12,10))
plt.scatter(vecs[:,1],vecs[:,2],c=Y,s=1)
#3D plot
plt.figure(figsize=(12,10))
ax = plt.axes(projection ="3d")
ax.scatter3D(vecs[:,1],vecs[:,2],vecs[:,3],c=Y,s=1)

# Sort eigenvalues and eigenvectors
sorted_indices = np.argsort(vals)
eigenvalues = vals[sorted_indices]
eigenvectors = vecs[:, sorted_indices]

# Step 6: Sort the values of the second smallest eigenvector
sorted_values = np.sort(eigenvectors[:, 1])

# Plot the sorted values
plt.plot(sorted_values, c='r')
plt.title("Sorted Values of Second Smallest Eigenvector")
plt.xlabel("Index")
plt.ylabel("Value")
plt.grid(True)
plt.show()

"""We now consider spectral clustering of pairs of MNIST digits."""

import numpy as np

#Binary clustering problem witih 2 digits
class1 = 5
class2 = 8

#Subset data to two digits
I = labels == class1
J = labels == class2
X = data[I | J,:]
L = labels[I | J]

#Convert labels to 0/1
I = L == class1
L[I] = 0
L[~I] = 1

#Build Graph (sparse 10-nearest neighbor graph)
W = gl.weightmatrix.knn(X,10)

#Spectral Clustering
spectral_labels = binary_spectral_clustering(W)
acc1 = np.mean(spectral_labels == L)
acc2 = np.mean(spectral_labels != L)
print('Spectral clustering accuracy = %.2f%%'%(100*max(acc1,acc2)))

#k-means clustering
kmeans_labels = kmeans(X,2)
acc1 = np.mean(kmeans_labels == L)
acc2 = np.mean(kmeans_labels != L)
print('K-means clustering accuracy = %.2f%%'%(100*max(acc1,acc2)))

#Show images from each cluster
gl.utils.image_grid(X[spectral_labels==0,:], n_rows=10, n_cols=10, title='Cluster 1', fontsize=26)
gl.utils.image_grid(X[spectral_labels==1,:], n_rows=10, n_cols=10, title='Cluster 2', fontsize=26)

"""# Exercise

Play around with the pairs of digits above. Are there any pairs for which k-means works better than spectral clustering?

Spectral clustering is not just a binary clustering algorithm. There are many extensions to multi-class clustering, and they all involve computing more eigenvectors of the graph Laplacian. The code below shows how to perform a spectral embedding of 4 digits from MNIST.
"""

from scipy import sparse
import matplotlib.pyplot as plt

#Subset data and labels
X = data[labels <= 3]
Y = labels[labels <= 3]

#Build Graph (sparse 10-nearest neighbor graph)
W = gl.weightmatrix.knn(X,10)
G = gl.graph(W)

#Compute eigenvectors of graph Laplacian
vals, vecs = G.eigen_decomp(normalization='normalized', k=20)

#Plot spectral embedding colored by label
#2D plot
plt.figure(figsize=(12,10))
plt.scatter(vecs[:,1],vecs[:,2],c=Y,s=1)
#3D plot
plt.figure(figsize=(12,10))
ax = plt.axes(projection ="3d")
ax.scatter3D(vecs[:,1],vecs[:,2],vecs[:,3],c=Y,s=1)

"""The code below runs spectral clustering on the whole MNIST dataset."""

import graphlearning as gl

#Load MNIST labels and results of k-nearest neighbor search
labels = gl.datasets.load('mnist', labels_only=True)
W = gl.weightmatrix.knn('mnist', 10, metric='vae')

#Run spectral clustering
model = gl.clustering.spectral(W, num_clusters=10, extra_dim=4)
cluster_labels = model.fit()

#Check accuracy
acc = gl.clustering.clustering_accuracy(cluster_labels,labels)
print('Accuracy=%.2f%%'%acc)

from scipy import sparse
import matplotlib.pyplot as plt

#Subset data and labels
X = data[labels <= 3]
Y = labels[labels <= 3]

#Build Graph (sparse 10-nearest neighbor graph)
W = gl.weightmatrix.knn(X,10)
G = gl.graph(W)

#Compute eigenvectors of graph Laplacian
vals, vecs = G.eigen_decomp(normalization='normalized', k=20)

#Plot spectral embedding colored by label
#2D plot
plt.figure(figsize=(12,10))
plt.scatter(vecs[:,1],vecs[:,2],c=Y,s=1)
#3D plot
plt.figure(figsize=(12,10))
ax = plt.axes(projection ="3d")
ax.scatter3D(vecs[:,1],vecs[:,2],vecs[:,3],c=Y,s=1)

# Sort eigenvalues and eigenvectors
sorted_indices = np.argsort(vals)
eigenvalues = vals[sorted_indices]
eigenvectors = vecs[:, sorted_indices]

# Plot the second smallest and second largest eigenvectors
plt.figure(figsize=(12, 6))
plt.subplot(121)
plt.title("Second Smallest Eigenvector")
plt.scatter(range(len(eigenvectors)), eigenvectors[:, 1], c='b', s=20)
plt.xlabel("Node Index")
plt.ylabel("Eigenvector Values")

plt.subplot(122)
plt.title("Eigenvalue")
plt.scatter(range(len(eigenvalues)), eigenvalues, c='r', s=20)
plt.xlabel("Node Index")
plt.ylabel("EigenvValues")

plt.tight_layout()
plt.show()

# Step 6: Sort the values of the second smallest eigenvector
sorted_values = np.sort(eigenvectors[:, 1])

# Plot the sorted values
plt.plot(sorted_values)
plt.title("Sorted Values of Second Smallest Eigenvector")
plt.xlabel("Index")
plt.ylabel("Value")
plt.grid(True)
plt.show()

from scipy import sparse
import matplotlib.pyplot as plt
#import graphlab as gl
#Subset data and labels
X = data[labels <= 3]
Y = labels[labels <= 3]

#Build Graph (sparse 10-nearest neighbor graph)
W = gl.weightmatrix.knn(X,10)
G = gl.graph(W)

#Compute eigenvectors of graph Laplacian
vals, vecs = G.eigen_decomp(k=20)

#Plot spectral embedding colored by label
#2D plot
plt.figure(figsize=(12,10))
plt.scatter(vecs[:,1],vecs[:,2],c=Y,s=1)
#3D plot
plt.figure(figsize=(12,10))
ax = plt.axes(projection ="3d")
ax.scatter3D(vecs[:,1],vecs[:,2],vecs[:,3],c=Y,s=1)

# Sort eigenvalues and eigenvectors
sorted_indices = np.argsort(vals)
eigenvalues = vals[sorted_indices]
eigenvectors = vecs[:, sorted_indices]

# Plot the second smallest and second largest eigenvectors
plt.figure(figsize=(12, 6))
plt.subplot(121)
plt.title("Second Smallest Eigenvector")
plt.scatter(range(len(eigenvectors)), eigenvectors[:, 1], c='b', s=20)
plt.xlabel("Node Index")
plt.ylabel("Eigenvector Values")


plt.subplot(122)
plt.title("Eigenvalue")
plt.scatter(range(len(eigenvalues)), eigenvalues, c='r', s=20)
plt.xlabel("Node Index")
plt.ylabel("EigenvValues")

plt.tight_layout()
plt.show()

# Step 6: Sort the values of the second smallest eigenvector
sorted_values = np.sort(eigenvectors[:, 1])

# Plot the sorted values
plt.plot(sorted_values)
plt.title("Sorted Values of Second Smallest Eigenvector")
plt.xlabel("Index")
plt.ylabel("Value")
plt.grid(True)
plt.show()

import numpy as np
import matplotlib.pyplot as plt
from scipy.sparse import csr_matrix
from sklearn.datasets import make_blobs
from sklearn.neighbors import kneighbors_graph
from sklearn.manifold import SpectralEmbedding
# Subset data and labels
X = data[labels <= 3]
Y = labels[labels <= 3]

# Build Graph (sparse 10-nearest neighbor graph)
W = kneighbors_graph(X, 10, mode='distance')
G = csr_matrix(W)

# Compute the normalized Laplacian matrix
degree_matrix = np.diag(np.array(G.sum(axis=1)).flatten())
normalized_laplacian = np.identity(G.shape[0]) - np.linalg.inv(degree_matrix) @ G

# Modify the Laplacian matrix as described
largest_row_sum = np.max(np.sum(normalized_laplacian, axis=1))
scaling_factor = largest_row_sum
identity_matrix = np.eye(normalized_laplacian.shape[0])
modified_laplacian = normalized_laplacian + scaling_factor * identity_matrix

# Convert the modified Laplacian matrix to a sparse CSR matrix if needed
modified_laplacian_sparse = csr_matrix(modified_laplacian)

# Compute the new condition number of the modified matrix
new_condition_number = np.linalg.cond(modified_laplacian)

# Compute the spectral embedding of the modified Laplacian matrix
spectral_embedding = SpectralEmbedding(n_components=3, affinity='precomputed')
vecs = spectral_embedding.fit_transform(modified_laplacian_sparse)

# Plot spectral embedding colored by label
# 2D plot
plt.figure(figsize=(12, 10))
plt.scatter(vecs[:, 0], vecs[:, 1], c=Y, s=30)

# 3D plot
plt.figure(figsize=(12, 10))
ax = plt.axes(projection="3d")
ax.scatter3D(vecs[:, 0], vecs[:, 1], vecs[:, 2], c=Y, s=30)

# Display information
print(f'Largest row sum: {largest_row_sum}')
print(f'Scaling factor: {scaling_factor}')
print(f'Condition number of modified matrix: {new_condition_number}')